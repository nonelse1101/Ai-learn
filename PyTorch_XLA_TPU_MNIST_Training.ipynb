{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nonelse1101/Ai-learn/blob/master/PyTorch_XLA_TPU_MNIST_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch-XLA TPU MNIST Training\n",
        "Demo"
      ],
      "metadata": {
        "id": "FFqRftgK7DPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This colab demo shows how to run distributed training on TPU for MNIST using PyTorch-XLA"
      ],
      "metadata": {
        "id": "7SaYJFvn8ECR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ],
      "metadata": {
        "id": "jZIGOzwPSGt2",
        "outputId": "9735291e-b62e-465c-8471-dc6f390a3678",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'COLAB_TPU_ADDR'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-54fd045d8168>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'COLAB_TPU_ADDR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'COLAB_TPU_ADDR'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install TPU compatible PyTorch"
      ],
      "metadata": {
        "id": "y4OQZUlE7Wqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cloud-tpu-client==0.10 torch==2.0.0 torchvision==0.15.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl\n",
        "# !pip3 install torch~=2.1.0 torchvision torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html\n",
        "# !pip install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html\n",
        "!pip install tensorboardX\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VfBSpIQm8zTH",
        "outputId": "11f4d84f-140c-427f-af3e-ca11696cdc1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-xla==2.0\n",
            "  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl (162.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.9/162.9 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cloud-tpu-client==0.10\n",
            "  Downloading cloud_tpu_client-0.10-py3-none-any.whl (7.4 kB)\n",
            "Collecting torch==2.0.0\n",
            "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.15.1\n",
            "  Downloading torchvision-0.15.1-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-api-python-client==1.8.0 (from cloud-tpu-client==0.10)\n",
            "  Downloading google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.1.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0 (from torch==2.0.0)\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.1) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.1) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.1) (9.4.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.22.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.17.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.1.1)\n",
            "Collecting google-api-core<2dev,>=1.13.0 (from google-api-python-client==1.8.0->cloud-tpu-client==0.10)\n",
            "  Downloading google_api_core-1.34.0-py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.2/120.2 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.16.0)\n",
            "Collecting uritemplate<4dev,>=3.0.0 (from google-api-python-client==1.8.0->cloud-tpu-client==0.10)\n",
            "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (0.41.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0) (3.27.7)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.0)\n",
            "  Downloading lit-17.0.3.tar.gz (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from torch-xla==2.0) (1.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0) (2.1.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.3.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->cloud-tpu-client==0.10) (4.9)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.1) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.1) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.61.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.20.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (5.3.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.1.1)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-17.0.3-py3-none-any.whl size=93258 sha256=cb9db43cf8bf4af8d1d2163d1b5fed470f57998b4fdeaf6e416e31db5a9d4af5\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/b8/42/f6f56aba870f9f3cc895b2e0c970ececaafc7d191217fa10a4\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, uritemplate, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, google-api-core, google-api-python-client, cloud-tpu-client, torch-xla, triton, torch, torchvision\n",
            "  Attempting uninstall: uritemplate\n",
            "    Found existing installation: uritemplate 4.1.1\n",
            "    Uninstalling uritemplate-4.1.1:\n",
            "      Successfully uninstalled uritemplate-4.1.1\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 2.11.1\n",
            "    Uninstalling google-api-core-2.11.1:\n",
            "      Successfully uninstalled google-api-core-2.11.1\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 2.84.0\n",
            "    Uninstalling google-api-python-client-2.84.0:\n",
            "      Successfully uninstalled google-api-python-client-2.84.0\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu118\n",
            "    Uninstalling torch-2.1.0+cu118:\n",
            "      Successfully uninstalled torch-2.1.0+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.16.0+cu118\n",
            "    Uninstalling torchvision-0.16.0+cu118:\n",
            "      Successfully uninstalled torchvision-0.16.0+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydrive2 1.6.3 requires google-api-python-client>=1.12.5, but you have google-api-python-client 1.8.0 which is incompatible.\n",
            "earthengine-api 0.1.375 requires google-api-python-client>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\n",
            "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.0.0 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.0 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cloud-tpu-client-0.10 google-api-core-1.34.0 google-api-python-client-1.8.0 lit-17.0.3 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0 torch-xla-2.0.0.dev20230516+colab torchvision-0.15.1 triton-2.0.0 uritemplate-3.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "googleapiclient",
                  "uritemplate"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tensorflow.python.profiler import profiler_client\n",
        "\n",
        "tpu_profile_service_address = os.environ['COLAB_TPU_ADDR'].replace('8470', '8466')\n",
        "print(profiler_client.monitor(tpu_profile_service_address, 100, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtsAmb0xQ6r3",
        "outputId": "feeec940-d78d-41c4-ae1c-1db50d450ed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Timestamp: 20:49:25\n",
            "  TPU type: TPU v2\n",
            "  Utilization of TPU Matrix Units (higher is better): 0.000%\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Torch and Torch Vision"
      ],
      "metadata": {
        "id": "bPnvktZU8OAv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_liQaW7U6Lhh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define parameters"
      ],
      "metadata": {
        "id": "JsUiO_Zl8rQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FLAGS = {}\n",
        "FLAGS['datadir'] = \"/data\"\n",
        "FLAGS['batch_size'] = 256\n",
        "FLAGS['learning_rate'] = 0.1\n",
        "FLAGS['momentum'] = 0.5\n",
        "FLAGS['num_epochs'] = 2\n",
        "FLAGS['num_workers'] = 4\n",
        "FLAGS['num_cores'] = 8\n",
        "FLAGS['log_steps'] = 20\n",
        "FLAGS['seed'] = 1"
      ],
      "metadata": {
        "id": "5qvKz5Pt0u8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Torch-XLA dependencies for distributed training"
      ],
      "metadata": {
        "id": "c8J8FZcn8bPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.utils.utils as xu\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.test.test_utils as test_utils\n",
        "import torch.distributed as dist"
      ],
      "metadata": {
        "id": "_iiQGLPM8l0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the model architecture"
      ],
      "metadata": {
        "id": "Mz6f1kEi87b0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(Net, self).__init__()\n",
        "      self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "      self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "      self.dropout1 = nn.Dropout(0.25)\n",
        "      self.dropout2 = nn.Dropout(0.5)\n",
        "      self.fc1 = nn.Linear(9216, 128)\n",
        "      self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.conv1(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.conv2(x)\n",
        "      x = F.relu(x)\n",
        "      x = F.max_pool2d(x, 2)\n",
        "      x = self.dropout1(x)\n",
        "      x = torch.flatten(x, 1)\n",
        "      x = self.fc1(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.dropout2(x)\n",
        "      x = self.fc2(x)\n",
        "      output = F.log_softmax(x, dim=1)\n",
        "      return output"
      ],
      "metadata": {
        "id": "E-Jd4qW00yna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Training Function"
      ],
      "metadata": {
        "id": "xqq9hNTf9Hbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def maintrain():\n",
        "\n",
        "  # summary writer\n",
        "  if xm.is_master_ordinal():\n",
        "    writer = test_utils.get_summary_writer('/tmp')\n",
        "\n",
        "  def _train_update(device, step, loss, tracker, epoch, writer):\n",
        "    test_utils.print_training_update(\n",
        "    device,\n",
        "    step,\n",
        "    loss.item(),\n",
        "    tracker.rate(),\n",
        "    tracker.global_rate(),\n",
        "    epoch,\n",
        "    summary_writer=writer)\n",
        "\n",
        "  torch.manual_seed(FLAGS['seed'])\n",
        "\n",
        "  # MNIST dataset preparation\n",
        "  print(\"preparing MNIST data\")\n",
        "  transform=transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.1307,), (0.3081,))\n",
        "      ])\n",
        "  train_dataset = datasets.MNIST(FLAGS['datadir'], train=True, download=True,\n",
        "                      transform=transform)\n",
        "  test_dataset = datasets.MNIST(FLAGS['datadir'], train=False,\n",
        "                      transform=transform)\n",
        "\n",
        "  train_kwargs = {'batch_size': FLAGS['batch_size'], 'drop_last': True}\n",
        "  test_kwargs = {'batch_size': FLAGS['batch_size']}\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset,**train_kwargs)\n",
        "  test_loader = torch.utils.data.DataLoader(test_dataset, **test_kwargs)\n",
        "\n",
        "  # get the device and port the model to the device (TPU)\n",
        "  device = xm.xla_device()\n",
        "  print(\"device\", device)\n",
        "  mp_device_loader = pl.MpDeviceLoader(train_loader, device)\n",
        "  mp_device_loader_test = pl.MpDeviceLoader(test_loader, device)\n",
        "  model = Net().to(device)\n",
        "\n",
        "  # get loss function, optimizer, and model\n",
        "  optimizer = optim.SGD(model.parameters(), lr=FLAGS['learning_rate'], momentum=FLAGS['momentum'])\n",
        "  loss_fn = nn.NLLLoss()\n",
        "\n",
        "  # define train loop\n",
        "  def train(model, train_loader, optimizer):\n",
        "    tracker = xm.RateTracker()\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "      optimizer.zero_grad()\n",
        "      output = model(data)\n",
        "      loss = loss_fn(output, target)\n",
        "      loss.backward()\n",
        "      xm.optimizer_step(optimizer)\n",
        "      tracker.add(FLAGS['batch_size'])\n",
        "\n",
        "      if batch_idx % FLAGS['log_steps'] == 0:\n",
        "        print('[xla:{}]({}) Loss={:.5f} Rate={:.2f} GlobalRate={:.2f} Time={}'.format(\n",
        "          xm.get_ordinal(), batch_idx, loss.item(), tracker.rate(),\n",
        "          tracker.global_rate(), time.asctime()), flush=True)\n",
        "\n",
        "  # define test loop\n",
        "  def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total_samples = 0\n",
        "    data, pred, target = None, None, None\n",
        "    with torch.no_grad():\n",
        "      for data, target in test_loader:\n",
        "        output = model(data)\n",
        "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        total_samples += data.size()[0]\n",
        "\n",
        "    # calculate accuracy after testing is done\n",
        "    accuracy = 100.0 * correct / total_samples\n",
        "    print('[xla:{}] Accuracy={:.2f}%'.format(\n",
        "        xm.get_ordinal(), accuracy), flush=True)\n",
        "    return accuracy, data, pred, target\n",
        "\n",
        "  # call train loop and perform training\n",
        "  for epoch in range(1, FLAGS['num_epochs'] + 1):\n",
        "    xm.master_print('Epoch {} train begin {}'.format(epoch, test_utils.now()))\n",
        "    train(model, mp_device_loader, optimizer)\n",
        "    xm.master_print('Epoch {} train end {}'.format(epoch, test_utils.now()))\n",
        "\n",
        "  # call test loop and perform testing\n",
        "  test(model, device, mp_device_loader_test)\n",
        "  test_utils.close_summary_writer(writer)\n"
      ],
      "metadata": {
        "id": "cTGkRtL5Ce8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training processes\n",
        "def _mp_fn(rank, flags):\n",
        "  global FLAGS\n",
        "  FLAGS = flags\n",
        "  torch.set_default_tensor_type('torch.FloatTensor')\n",
        "  maintrain()\n",
        "\n",
        "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS['num_cores'], start_method='fork')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ETQvpp8x0U3z",
        "outputId": "b7dc5b58-8a12-4041-f6e0-42caa34af76b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preparing MNIST data\n",
            "device xla:1\n",
            "Epoch 1 train begin 22:31:22\n",
            "[xla:0](0) Loss=2.31219 Rate=214.92 GlobalRate=214.91 Time=Wed Oct 25 22:31:23 2023\n",
            "preparing MNIST data\n",
            "preparing MNIST data\n",
            "device xla:0\n",
            "device xla:0\n",
            "preparing MNIST data\n",
            "preparing MNIST data\n",
            "device devicexla:0 \n",
            "xla:0\n",
            "preparing MNIST data\n",
            "preparing MNIST data\n",
            "device xla:0\n",
            "[xla:7](0) Loss=2.31219 Rate=124.78 GlobalRate=124.78 Time=Wed Oct 25 22:31:28 2023\n",
            "[xla:2](0) Loss=2.31219 Rate=123.43 GlobalRate=123.43 Time=Wed Oct 25 22:31:28 2023\n",
            "preparing MNIST data\n",
            "device xla:0\n",
            "device xla:0\n",
            "[xla:4](0) Loss=2.31219 Rate=84.48 GlobalRate=84.48 Time=Wed Oct 25 22:31:30 2023\n",
            "[xla:3](0) Loss=2.31219 Rate=81.53 GlobalRate=81.53 Time=Wed Oct 25 22:31:30 2023\n",
            "[xla:6](0) Loss=2.31219 Rate=115.08 GlobalRate=115.08 Time=Wed Oct 25 22:31:31 2023\n",
            "[xla:1](0) Loss=2.31219 Rate=113.26 GlobalRate=113.25 Time=Wed Oct 25 22:31:31 2023\n",
            "[xla:5](0) Loss=2.31219 Rate=121.01 GlobalRate=121.01 Time=Wed Oct 25 22:31:32 2023\n",
            "[xla:0](20) Loss=0.92895 Rate=281.80 GlobalRate=318.52 Time=Wed Oct 25 22:31:39 2023[xla:7](20) Loss=0.92895 Rate=323.48 GlobalRate=404.79 Time=Wed Oct 25 22:31:39 2023[xla:2](20) Loss=0.92895 Rate=323.77 GlobalRate=405.15 Time=Wed Oct 25 22:31:39 2023[xla:4](20) Loss=0.92895 Rate=383.78 GlobalRate=455.23 Time=Wed Oct 25 22:31:39 2023\n",
            "\n",
            "\n",
            "\n",
            "[xla:3](20) Loss=0.92895 Rate=379.21 GlobalRate=447.88 Time=Wed Oct 25 22:31:39 2023\n",
            "[xla:6](20) Loss=0.92895 Rate=399.11 GlobalRate=492.07 Time=Wed Oct 25 22:31:39 2023\n",
            "[xla:1](20) Loss=0.92895 Rate=398.24 GlobalRate=490.31 Time=Wed Oct 25 22:31:40 2023\n",
            "[xla:5](20) Loss=0.92895 Rate=380.98 GlobalRate=473.55 Time=Wed Oct 25 22:31:41 2023\n",
            "[xla:7](40) Loss=0.42411 Rate=343.10 GlobalRate=379.53 Time=Wed Oct 25 22:31:53 2023[xla:4](40) Loss=0.42411 Rate=367.19 GlobalRate=400.85 Time=Wed Oct 25 22:31:53 2023\n",
            "[xla:2](40) Loss=0.42411 Rate=342.84 GlobalRate=379.33 Time=Wed Oct 25 22:31:53 2023\n",
            "\n",
            "[xla:0](40) Loss=0.42411 Rate=325.38 GlobalRate=335.08 Time=Wed Oct 25 22:31:53 2023\n",
            "[xla:3](40) Loss=0.42411 Rate=365.15 GlobalRate=397.66 Time=Wed Oct 25 22:31:54 2023\n",
            "[xla:1](40) Loss=0.42411 Rate=380.30 GlobalRate=422.12 Time=Wed Oct 25 22:31:54 2023[xla:6](40) Loss=0.42411 Rate=372.74 GlobalRate=414.19 Time=Wed Oct 25 22:31:54 2023\n",
            "\n",
            "[xla:5](40) Loss=0.42411 Rate=373.09 GlobalRate=415.32 Time=Wed Oct 25 22:31:55 2023\n",
            "[xla:7](60) Loss=0.29407 Rate=505.67 GlobalRate=433.85 Time=Wed Oct 25 22:32:02 2023\n",
            "[xla:0](60) Loss=0.29407 Rate=500.50 GlobalRate=394.16 Time=Wed Oct 25 22:32:02 2023[xla:2](60) Loss=0.29407 Rate=506.01 GlobalRate=433.80 Time=Wed Oct 25 22:32:02 2023[xla:4](60) Loss=0.29407 Rate=513.93 GlobalRate=451.93 Time=Wed Oct 25 22:32:02 2023\n",
            "\n",
            "\n",
            "[xla:3](60) Loss=0.29407 Rate=484.23 GlobalRate=440.15 Time=Wed Oct 25 22:32:03 2023\n",
            "[xla:6](60) Loss=0.29407 Rate=485.23 GlobalRate=452.89 Time=Wed Oct 25 22:32:03 2023\n",
            "[xla:1](60) Loss=0.29407 Rate=479.92 GlobalRate=456.12 Time=Wed Oct 25 22:32:03 2023\n",
            "[xla:5](60) Loss=0.29407 Rate=515.63 GlobalRate=463.98 Time=Wed Oct 25 22:32:03 2023\n",
            "[xla:2](80) Loss=0.28569 Rate=418.53 GlobalRate=412.97 Time=Wed Oct 25 22:32:16 2023[xla:7](80) Loss=0.28569 Rate=417.86 GlobalRate=412.71 Time=Wed Oct 25 22:32:16 2023[xla:0](80) Loss=0.28569 Rate=415.90 GlobalRate=384.99 Time=Wed Oct 25 22:32:16 2023\n",
            "\n",
            "\n",
            "[xla:4](80) Loss=0.28569 Rate=415.09 GlobalRate=421.32 Time=Wed Oct 25 22:32:16 2023\n",
            "[xla:3](80) Loss=0.28569 Rate=398.18 GlobalRate=410.60 Time=Wed Oct 25 22:32:18 2023\n",
            "[xla:6](80) Loss=0.28569 Rate=400.76 GlobalRate=420.23 Time=Wed Oct 25 22:32:18 2023[xla:1](80) Loss=0.28569 Rate=401.77 GlobalRate=424.23 Time=Wed Oct 25 22:32:18 2023\n",
            "\n",
            "[xla:5](80) Loss=0.28569 Rate=414.33 GlobalRate=428.25 Time=Wed Oct 25 22:32:18 2023\n",
            "[xla:4](100) Loss=0.23678 Rate=503.53 GlobalRate=443.36 Time=Wed Oct 25 22:32:25 2023\n",
            "[xla:2](100) Loss=0.23678 Rate=487.98 GlobalRate=432.41 Time=Wed Oct 25 22:32:25 2023[xla:0](100) Loss=0.23678 Rate=487.71 GlobalRate=407.69 Time=Wed Oct 25 22:32:25 2023[xla:7](100) Loss=0.23678 Rate=486.92 GlobalRate=432.02 Time=Wed Oct 25 22:32:25 2023\n",
            "\n",
            "\n",
            "[xla:3](100) Loss=0.23678 Rate=500.23 GlobalRate=434.47 Time=Wed Oct 25 22:32:27 2023\n",
            "[xla:6](100) Loss=0.23678 Rate=503.81 GlobalRate=443.59 Time=Wed Oct 25 22:32:27 2023\n",
            "[xla:5](100) Loss=0.23678 Rate=507.28 GlobalRate=450.34 Time=Wed Oct 25 22:32:27 2023\n",
            "[xla:1](100) Loss=0.23678 Rate=479.17 GlobalRate=441.79 Time=Wed Oct 25 22:32:27 2023\n",
            "[xla:2](120) Loss=0.18749 Rate=416.64 GlobalRate=420.49 Time=Wed Oct 25 22:32:39 2023[xla:7](120) Loss=0.18749 Rate=416.58 GlobalRate=420.30 Time=Wed Oct 25 22:32:39 2023[xla:4](120) Loss=0.18749 Rate=422.20 GlobalRate=428.84 Time=Wed Oct 25 22:32:39 2023\n",
            "[xla:0](120) Loss=0.18749 Rate=416.20 GlobalRate=400.65 Time=Wed Oct 25 22:32:39 2023\n",
            "\n",
            "\n",
            "[xla:6](120) Loss=0.18749 Rate=432.51 GlobalRate=432.70 Time=Wed Oct 25 22:32:40 2023\n",
            "[xla:3](120) Loss=0.18749 Rate=421.72 GlobalRate=422.17 Time=Wed Oct 25 22:32:41 2023\n",
            "[xla:5](120) Loss=0.18749 Rate=428.09 GlobalRate=435.93 Time=Wed Oct 25 22:32:41 2023\n",
            "[xla:1](120) Loss=0.18749 Rate=414.66 GlobalRate=428.43 Time=Wed Oct 25 22:32:41 2023\n",
            "[xla:2](140) Loss=0.19156 Rate=494.04 GlobalRate=434.63 Time=Wed Oct 25 22:32:49 2023[xla:7](140) Loss=0.19156 Rate=494.19 GlobalRate=434.48 Time=Wed Oct 25 22:32:49 2023[xla:0](140) Loss=0.19156 Rate=494.42 GlobalRate=416.42 Time=Wed Oct 25 22:32:49 2023\n",
            "\n",
            "[xla:4](140) Loss=0.19156 Rate=496.42 GlobalRate=442.29 Time=Wed Oct 25 22:32:49 2023\n",
            "[xla:6](140) Loss=0.19156 Rate=526.39 GlobalRate=449.62 Time=Wed Oct 25 22:32:49 2023\n",
            "\n",
            "[xla:3](140) Loss=0.19156 Rate=537.67 GlobalRate=441.82 Time=Wed Oct 25 22:32:49 2023\n",
            "[xla:5](140) Loss=0.19156 Rate=510.79 GlobalRate=450.61 Time=Wed Oct 25 22:32:50 2023\n",
            "[xla:1](140) Loss=0.19156 Rate=490.35 GlobalRate=441.44 Time=Wed Oct 25 22:32:51 2023\n",
            "[xla:7](160) Loss=0.17754 Rate=424.33 GlobalRate=426.53 Time=Wed Oct 25 22:33:02 2023[xla:4](160) Loss=0.17754 Rate=425.19 GlobalRate=433.09 Time=Wed Oct 25 22:33:02 2023\n",
            "[xla:6](160) Loss=0.17754 Rate=437.63 GlobalRate=439.36 Time=Wed Oct 25 22:33:02 2023\n",
            "\n",
            "[xla:0](160) Loss=0.17754 Rate=423.36 GlobalRate=410.93 Time=Wed Oct 25 22:33:02 2023\n",
            "[xla:2](160) Loss=0.17754 Rate=422.50 GlobalRate=426.18 Time=Wed Oct 25 22:33:02 2023\n",
            "[xla:3](160) Loss=0.17754 Rate=440.35 GlobalRate=432.33 Time=Wed Oct 25 22:33:02 2023\n",
            "[xla:5](160) Loss=0.17754 Rate=422.27 GlobalRate=437.54 Time=Wed Oct 25 22:33:04 2023\n",
            "[xla:1](160) Loss=0.17754 Rate=419.85 GlobalRate=431.58 Time=Wed Oct 25 22:33:04 2023\n",
            "[xla:4](180) Loss=0.26748 Rate=486.25 GlobalRate=441.79 Time=Wed Oct 25 22:33:12 2023[xla:7](180) Loss=0.26748 Rate=484.80 GlobalRate=435.56 Time=Wed Oct 25 22:33:12 2023\n",
            "[xla:2](180) Loss=0.26748 Rate=487.45 GlobalRate=435.66 Time=Wed Oct 25 22:33:12 2023[xla:0](180) Loss=0.26748 Rate=486.21 GlobalRate=421.26 Time=Wed Oct 25 22:33:12 2023[xla:6](180) Loss=0.26748 Rate=490.87 GlobalRate=447.53 Time=Wed Oct 25 22:33:12 2023[xla:3](180) Loss=0.26748 Rate=497.02 GlobalRate=441.68 Time=Wed Oct 25 22:33:12 2023\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[xla:5](180) Loss=0.26748 Rate=520.06 GlobalRate=450.09 Time=Wed Oct 25 22:33:12 2023\n",
            "[xla:1](180) Loss=0.26748 Rate=482.76 GlobalRate=440.21 Time=Wed Oct 25 22:33:14 2023\n",
            "[xla:2](200) Loss=0.20425 Rate=431.39 GlobalRate=431.13 Time=Wed Oct 25 22:33:25 2023[xla:7](200) Loss=0.20425 Rate=430.24 GlobalRate=431.02 Time=Wed Oct 25 22:33:25 2023\n",
            "[xla:0](200) Loss=0.20425 Rate=430.68 GlobalRate=418.34 Time=Wed Oct 25 22:33:25 2023[xla:4](200) Loss=0.20425 Rate=429.91 GlobalRate=436.32 Time=Wed Oct 25 22:33:25 2023\n",
            "\n",
            "\n",
            "[xla:6](200) Loss=0.20425 Rate=430.09 GlobalRate=441.00 Time=Wed Oct 25 22:33:25 2023\n",
            "[xla:3](200) Loss=0.20425 Rate=416.94 GlobalRate=432.44 Time=Wed Oct 25 22:33:26 2023\n",
            "[xla:5](200) Loss=0.20425 Rate=427.46 GlobalRate=439.99 Time=Wed Oct 25 22:33:26 2023\n",
            "[xla:1](200) Loss=0.20425 Rate=430.22 GlobalRate=435.28 Time=Wed Oct 25 22:33:27 2023\n",
            "[xla:7](220) Loss=0.13538 Rate=506.65 GlobalRate=440.06 Time=Wed Oct 25 22:33:34 2023[xla:0](220) Loss=0.13538 Rate=507.08 GlobalRate=428.04 Time=Wed Oct 25 22:33:34 2023[xla:2](220) Loss=0.13538 Rate=506.29 GlobalRate=440.08 Time=Wed Oct 25 22:33:34 2023[xla:4](220) Loss=0.13538 Rate=507.12 GlobalRate=445.14 Time=Wed Oct 25 22:33:34 2023\n",
            "\n",
            "\n",
            "\n",
            "[xla:6](220) Loss=0.13538 Rate=506.02 GlobalRate=449.45 Time=Wed Oct 25 22:33:34 2023\n",
            "[xla:3](220) Loss=0.13538 Rate=518.98 GlobalRate=442.99 Time=Wed Oct 25 22:33:35 2023\n",
            "[xla:5](220) Loss=0.13538 Rate=512.41 GlobalRate=449.21 Time=Wed Oct 25 22:33:35 2023\n",
            "[xla:1](220) Loss=0.13538 Rate=525.97 GlobalRate=445.85 Time=Wed Oct 25 22:33:36 2023\n",
            "Epoch 1 train end 22:33:39\n",
            "Epoch 2 train begin 22:33:39\n",
            "[xla:0](0) Loss=0.19897 Rate=102.17 GlobalRate=102.17 Time=Wed Oct 25 22:33:42 2023[xla:7](0) Loss=0.19897 Rate=98.28 GlobalRate=98.28 Time=Wed Oct 25 22:33:42 2023\n",
            "\n",
            "[xla:5](0) Loss=0.19897 Rate=94.58 GlobalRate=94.58 Time=Wed Oct 25 22:33:42 2023\n",
            "[xla:2](0) Loss=0.19897 Rate=91.70 GlobalRate=91.70 Time=Wed Oct 25 22:33:42 2023[xla:6](0) Loss=0.19897 Rate=90.68 GlobalRate=90.68 Time=Wed Oct 25 22:33:42 2023\n",
            "\n",
            "[xla:4](0) Loss=0.19897 Rate=87.00 GlobalRate=87.00 Time=Wed Oct 25 22:33:42 2023\n",
            "[xla:1](0) Loss=0.19897 Rate=85.44 GlobalRate=85.44 Time=Wed Oct 25 22:33:42 2023[xla:3](0) Loss=0.19897 Rate=85.49 GlobalRate=85.49 Time=Wed Oct 25 22:33:42 2023\n",
            "\n",
            "[xla:6](20) Loss=0.17939 Rate=323.56 GlobalRate=397.75 Time=Wed Oct 25 22:33:53 2023\n",
            "[xla:4](20) Loss=0.17939 Rate=311.17 GlobalRate=382.41 Time=Wed Oct 25 22:33:53 2023\n",
            "[xla:0](20) Loss=0.17939 Rate=306.29 GlobalRate=381.83 Time=Wed Oct 25 22:33:54 2023\n",
            "[xla:2](20) Loss=0.17939 Rate=304.80 GlobalRate=377.28 Time=Wed Oct 25 22:33:54 2023\n",
            "[xla:7](20) Loss=0.17939 Rate=296.95 GlobalRate=370.03 Time=Wed Oct 25 22:33:54 2023\n",
            "[xla:5](20) Loss=0.17939 Rate=297.24 GlobalRate=369.51 Time=Wed Oct 25 22:33:54 2023\n",
            "[xla:3](20) Loss=0.17939 Rate=293.99 GlobalRate=362.77 Time=Wed Oct 25 22:33:54 2023\n",
            "[xla:1](20) Loss=0.17939 Rate=290.13 GlobalRate=358.44 Time=Wed Oct 25 22:33:54 2023\n",
            "[xla:6](40) Loss=0.14566 Rate=423.31 GlobalRate=437.90 Time=Wed Oct 25 22:34:03 2023\n",
            "[xla:4](40) Loss=0.14566 Rate=405.42 GlobalRate=419.97 Time=Wed Oct 25 22:34:04 2023[xla:7](40) Loss=0.14566 Rate=412.28 GlobalRate=419.92 Time=Wed Oct 25 22:34:04 2023\n",
            "\n",
            "[xla:0](40) Loss=0.14566 Rate=395.17 GlobalRate=414.10 Time=Wed Oct 25 22:34:05 2023\n",
            "[xla:3](40) Loss=0.14566 Rate=395.74 GlobalRate=405.82 Time=Wed Oct 25 22:34:05 2023\n",
            "[xla:1](40) Loss=0.14566 Rate=388.13 GlobalRate=399.25 Time=Wed Oct 25 22:34:06 2023\n",
            "[xla:5](40) Loss=0.14566 Rate=379.01 GlobalRate=398.19 Time=Wed Oct 25 22:34:06 2023\n",
            "[xla:2](40) Loss=0.14566 Rate=373.73 GlobalRate=396.84 Time=Wed Oct 25 22:34:06 2023\n",
            "[xla:6](60) Loss=0.12983 Rate=416.46 GlobalRate=429.02 Time=Wed Oct 25 22:34:16 2023[xla:4](60) Loss=0.12983 Rate=431.27 GlobalRate=428.91 Time=Wed Oct 25 22:34:16 2023\n",
            "\n",
            "[xla:7](60) Loss=0.12983 Rate=428.44 GlobalRate=426.06 Time=Wed Oct 25 22:34:16 2023\n",
            "[xla:3](60) Loss=0.12983 Rate=429.34 GlobalRate=419.81 Time=Wed Oct 25 22:34:17 2023\n",
            "[xla:5](60) Loss=0.12983 Rate=434.06 GlobalRate=419.39 Time=Wed Oct 25 22:34:17 2023\n",
            "[xla:0](60) Loss=0.12983 Rate=417.49 GlobalRate=419.91 Time=Wed Oct 25 22:34:17 2023\n",
            "[xla:1](60) Loss=0.12983 Rate=424.38 GlobalRate=414.18 Time=Wed Oct 25 22:34:17 2023\n",
            "[xla:2](60) Loss=0.12983 Rate=417.44 GlobalRate=411.88 Time=Wed Oct 25 22:34:17 2023\n",
            "[xla:6](80) Loss=0.10888 Rate=464.49 GlobalRate=443.92 Time=Wed Oct 25 22:34:26 2023\n",
            "[xla:7](80) Loss=0.10888 Rate=468.21 GlobalRate=441.17 Time=Wed Oct 25 22:34:26 2023\n",
            "[xla:4](80) Loss=0.10888 Rate=445.17 GlobalRate=434.95 Time=Wed Oct 25 22:34:27 2023\n",
            "[xla:5](80) Loss=0.10888 Rate=449.87 GlobalRate=428.82 Time=Wed Oct 25 22:34:28 2023[xla:3](80) Loss=0.10888 Rate=446.85 GlobalRate=428.75 Time=Wed Oct 25 22:34:28 2023\n",
            "[xla:0](80) Loss=0.10888 Rate=442.32 GlobalRate=428.91 Time=Wed Oct 25 22:34:28 2023\n",
            "\n",
            "[xla:1](80) Loss=0.10888 Rate=438.05 GlobalRate=421.86 Time=Wed Oct 25 22:34:29 2023\n",
            "[xla:2](80) Loss=0.10888 Rate=433.35 GlobalRate=419.36 Time=Wed Oct 25 22:34:29 2023\n",
            "[xla:6](100) Loss=0.17282 Rate=437.74 GlobalRate=438.95 Time=Wed Oct 25 22:34:38 2023\n",
            "[xla:4](100) Loss=0.17282 Rate=447.05 GlobalRate=437.53 Time=Wed Oct 25 22:34:38 2023\n",
            "[xla:7](100) Loss=0.17282 Rate=433.19 GlobalRate=434.60 Time=Wed Oct 25 22:34:39 2023\n",
            "[xla:5](100) Loss=0.17282 Rate=442.16 GlobalRate=430.42 Time=Wed Oct 25 22:34:39 2023\n",
            "[xla:2](100) Loss=0.17282 Rate=456.11 GlobalRate=428.71 Time=Wed Oct 25 22:34:40 2023\n",
            "[xla:3](100) Loss=0.17282 Rate=434.41 GlobalRate=428.22 Time=Wed Oct 25 22:34:40 2023\n",
            "[xla:1](100) Loss=0.17282 Rate=446.16 GlobalRate=427.43 Time=Wed Oct 25 22:34:40 2023\n",
            "[xla:0](100) Loss=0.17282 Rate=428.35 GlobalRate=426.91 Time=Wed Oct 25 22:34:40 2023\n",
            "[xla:6](120) Loss=0.09659 Rate=464.80 GlobalRate=445.64 Time=Wed Oct 25 22:34:49 2023\n",
            "[xla:4](120) Loss=0.09659 Rate=452.56 GlobalRate=440.51 Time=Wed Oct 25 22:34:50 2023\n",
            "[xla:7](120) Loss=0.09659 Rate=456.22 GlobalRate=440.30 Time=Wed Oct 25 22:34:50 2023\n",
            "[xla:5](120) Loss=0.09659 Rate=460.13 GlobalRate=436.80 Time=Wed Oct 25 22:34:50 2023\n",
            "[xla:0](120) Loss=0.09659 Rate=450.18 GlobalRate=432.74 Time=Wed Oct 25 22:34:51 2023\n",
            "[xla:3](120) Loss=0.09659 Rate=442.97 GlobalRate=431.47 Time=Wed Oct 25 22:34:51 2023\n",
            "[xla:2](120) Loss=0.09659 Rate=447.52 GlobalRate=430.82 Time=Wed Oct 25 22:34:51 2023\n",
            "[xla:1](120) Loss=0.09659 Rate=439.22 GlobalRate=428.60 Time=Wed Oct 25 22:34:52 2023\n",
            "[xla:6](140) Loss=0.14246 Rate=437.55 GlobalRate=441.72 Time=Wed Oct 25 22:35:01 2023\n",
            "[xla:7](140) Loss=0.14246 Rate=449.57 GlobalRate=440.98 Time=Wed Oct 25 22:35:01 2023\n",
            "[xla:4](140) Loss=0.14246 Rate=439.15 GlobalRate=439.02 Time=Wed Oct 25 22:35:02 2023\n",
            "[xla:0](140) Loss=0.14246 Rate=465.18 GlobalRate=438.29 Time=Wed Oct 25 22:35:02 2023\n",
            "[xla:5](140) Loss=0.14246 Rate=448.54 GlobalRate=437.36 Time=Wed Oct 25 22:35:02 2023\n",
            "[xla:1](140) Loss=0.14246 Rate=468.90 GlobalRate=436.20 Time=Wed Oct 25 22:35:02 2023\n",
            "[xla:2](140) Loss=0.14246 Rate=456.14 GlobalRate=434.97 Time=Wed Oct 25 22:35:02 2023\n",
            "[xla:3](140) Loss=0.14246 Rate=442.60 GlobalRate=432.98 Time=Wed Oct 25 22:35:03 2023\n",
            "[xla:6](160) Loss=0.11569 Rate=481.47 GlobalRate=449.26 Time=Wed Oct 25 22:35:11 2023\n",
            "[xla:7](160) Loss=0.11569 Rate=470.46 GlobalRate=445.95 Time=Wed Oct 25 22:35:12 2023\n",
            "[xla:4](160) Loss=0.11569 Rate=468.64 GlobalRate=444.60 Time=Wed Oct 25 22:35:12 2023\n",
            "[xla:5](160) Loss=0.11569 Rate=466.71 GlobalRate=442.12 Time=Wed Oct 25 22:35:13 2023\n",
            "[xla:0](160) Loss=0.11569 Rate=470.15 GlobalRate=442.37 Time=Wed Oct 25 22:35:13 2023\n",
            "[xla:1](160) Loss=0.11569 Rate=469.56 GlobalRate=440.14 Time=Wed Oct 25 22:35:13 2023\n",
            "[xla:3](160) Loss=0.11569 Rate=446.97 GlobalRate=435.01 Time=Wed Oct 25 22:35:14 2023[xla:2](160) Loss=0.11569 Rate=443.41 GlobalRate=434.96 Time=Wed Oct 25 22:35:14 2023\n",
            "\n",
            "[xla:6](180) Loss=0.18838 Rate=434.61 GlobalRate=443.69 Time=Wed Oct 25 22:35:24 2023\n",
            "[xla:4](180) Loss=0.18838 Rate=444.56 GlobalRate=442.76 Time=Wed Oct 25 22:35:24 2023\n",
            "[xla:7](180) Loss=0.18838 Rate=429.29 GlobalRate=440.60 Time=Wed Oct 25 22:35:25 2023[xla:1](180) Loss=0.18838 Rate=454.20 GlobalRate=440.55 Time=Wed Oct 25 22:35:25 2023\n",
            "\n",
            "[xla:5](180) Loss=0.18838 Rate=440.83 GlobalRate=439.99 Time=Wed Oct 25 22:35:25 2023\n",
            "[xla:0](180) Loss=0.18838 Rate=440.12 GlobalRate=439.80 Time=Wed Oct 25 22:35:25 2023\n",
            "[xla:3](180) Loss=0.18838 Rate=460.21 GlobalRate=438.53 Time=Wed Oct 25 22:35:25 2023\n",
            "[xla:2](180) Loss=0.18838 Rate=448.47 GlobalRate=436.77 Time=Wed Oct 25 22:35:25 2023\n",
            "[xla:6](200) Loss=0.14446 Rate=487.20 GlobalRate=450.43 Time=Wed Oct 25 22:35:34 2023\n",
            "[xla:4](200) Loss=0.14446 Rate=468.60 GlobalRate=446.60 Time=Wed Oct 25 22:35:35 2023\n",
            "[xla:7](200) Loss=0.14446 Rate=459.19 GlobalRate=444.16 Time=Wed Oct 25 22:35:35 2023\n",
            "[xla:1](200) Loss=0.14446 Rate=461.88 GlobalRate=443.05 Time=Wed Oct 25 22:35:36 2023\n",
            "[xla:5](200) Loss=0.14446 Rate=458.07 GlobalRate=442.76 Time=Wed Oct 25 22:35:36 2023\n",
            "[xla:2](200) Loss=0.14446 Rate=476.54 GlobalRate=441.96 Time=Wed Oct 25 22:35:36 2023\n",
            "[xla:0](200) Loss=0.14446 Rate=448.64 GlobalRate=441.20 Time=Wed Oct 25 22:35:36 2023\n",
            "[xla:3](200) Loss=0.14446 Rate=451.61 GlobalRate=439.25 Time=Wed Oct 25 22:35:37 2023\n",
            "[xla:6](220) Loss=0.09709 Rate=418.15 GlobalRate=442.01 Time=Wed Oct 25 22:35:47 2023\n",
            "[xla:4](220) Loss=0.09709 Rate=421.59 GlobalRate=440.84 Time=Wed Oct 25 22:35:48 2023\n",
            "[xla:7](220) Loss=0.09709 Rate=421.65 GlobalRate=439.39 Time=Wed Oct 25 22:35:48 2023\n",
            "[xla:1](220) Loss=0.09709 Rate=426.77 GlobalRate=439.14 Time=Wed Oct 25 22:35:48 2023\n",
            "[xla:2](220) Loss=0.09709 Rate=436.84 GlobalRate=438.90 Time=Wed Oct 25 22:35:48 2023\n",
            "[xla:0](220) Loss=0.09709 Rate=423.41 GlobalRate=437.83 Time=Wed Oct 25 22:35:49 2023[xla:3](220) Loss=0.09709 Rate=432.84 GlobalRate=437.47 Time=Wed Oct 25 22:35:49 2023\n",
            "\n",
            "[xla:5](220) Loss=0.09709 Rate=409.54 GlobalRate=435.90 Time=Wed Oct 25 22:35:49 2023\n",
            "Epoch 2 train end 22:35:53\n",
            "[xla:1] Accuracy=98.27%[xla:7] Accuracy=98.27%[xla:2] Accuracy=98.27%[xla:5] Accuracy=98.27%\n",
            "[xla:3] Accuracy=98.27%[xla:4] Accuracy=98.27%[xla:6] Accuracy=98.27%[xla:0] Accuracy=98.27%\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in device=TPU:6: local variable 'writer' referenced before assignmentException in device=TPU:4: local variable 'writer' referenced before assignmentException in device=TPU:3: local variable 'writer' referenced before assignment\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Exception in device=TPU:2: local variable 'writer' referenced before assignmentTraceback (most recent call last):\n",
            "Exception in device=TPU:7: local variable 'writer' referenced before assignmentTraceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "\n",
            "\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 334, in _mp_start_fn\n",
            "    _start_fn(index, pf_cfg, fn, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 334, in _mp_start_fn\n",
            "    _start_fn(index, pf_cfg, fn, args)\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 328, in _start_fn\n",
            "    fn(gindex, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 328, in _start_fn\n",
            "    fn(gindex, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 334, in _mp_start_fn\n",
            "    _start_fn(index, pf_cfg, fn, args)\n",
            "  File \"<ipython-input-8-47ee2ab6101b>\", line 6, in _mp_fn\n",
            "    maintrain()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 334, in _mp_start_fn\n",
            "    _start_fn(index, pf_cfg, fn, args)\n",
            "  File \"<ipython-input-8-47ee2ab6101b>\", line 6, in _mp_fn\n",
            "    maintrain()\n",
            "Exception in device=TPU:1: local variable 'writer' referenced before assignment  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 328, in _start_fn\n",
            "    fn(gindex, *args)\n",
            "  File \"<ipython-input-7-4400f42ec4a6>\", line 90, in maintrain\n",
            "    test_utils.close_summary_writer(writer)\n",
            "Exception in device=TPU:5: local variable 'writer' referenced before assignment  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 328, in _start_fn\n",
            "    fn(gindex, *args)\n",
            "  File \"<ipython-input-7-4400f42ec4a6>\", line 90, in maintrain\n",
            "    test_utils.close_summary_writer(writer)\n",
            "  File \"<ipython-input-8-47ee2ab6101b>\", line 6, in _mp_fn\n",
            "    maintrain()\n",
            "UnboundLocalError: local variable 'writer' referenced before assignment\n",
            "\n",
            "\n",
            "  File \"<ipython-input-8-47ee2ab6101b>\", line 6, in _mp_fn\n",
            "    maintrain()\n",
            "UnboundLocalError: local variable 'writer' referenced before assignment\n",
            "  File \"<ipython-input-7-4400f42ec4a6>\", line 90, in maintrain\n",
            "    test_utils.close_summary_writer(writer)\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-7-4400f42ec4a6>\", line 90, in maintrain\n",
            "    test_utils.close_summary_writer(writer)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 334, in _mp_start_fn\n",
            "    _start_fn(index, pf_cfg, fn, args)\n",
            "UnboundLocalError: local variable 'writer' referenced before assignment\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 328, in _start_fn\n",
            "    fn(gindex, *args)\n",
            "  File \"<ipython-input-8-47ee2ab6101b>\", line 6, in _mp_fn\n",
            "    maintrain()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 334, in _mp_start_fn\n",
            "    _start_fn(index, pf_cfg, fn, args)\n",
            "UnboundLocalError: local variable 'writer' referenced before assignment\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 328, in _start_fn\n",
            "    fn(gindex, *args)\n",
            "  File \"<ipython-input-7-4400f42ec4a6>\", line 90, in maintrain\n",
            "    test_utils.close_summary_writer(writer)\n",
            "UnboundLocalError: local variable 'writer' referenced before assignment\n",
            "  File \"<ipython-input-8-47ee2ab6101b>\", line 6, in _mp_fn\n",
            "    maintrain()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 334, in _mp_start_fn\n",
            "    _start_fn(index, pf_cfg, fn, args)\n",
            "  File \"<ipython-input-7-4400f42ec4a6>\", line 90, in maintrain\n",
            "    test_utils.close_summary_writer(writer)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 328, in _start_fn\n",
            "    fn(gindex, *args)\n",
            "  File \"<ipython-input-8-47ee2ab6101b>\", line 6, in _mp_fn\n",
            "    maintrain()\n",
            "  File \"<ipython-input-7-4400f42ec4a6>\", line 90, in maintrain\n",
            "    test_utils.close_summary_writer(writer)\n",
            "UnboundLocalError: local variable 'writer' referenced before assignment\n",
            "UnboundLocalError: local variable 'writer' referenced before assignment\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ProcessExitedException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-47ee2ab6101b>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mmaintrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mxmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_mp_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_cores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fork'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0m_start_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpf_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m     result = torch.multiprocessing.start_processes(\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0m_mp_start_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpf_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m                 )\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 raise ProcessExitedException(\n\u001b[0m\u001b[1;32m    150\u001b[0m                     \u001b[0;34m\"process %d terminated with exit code %d\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0merror_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mProcessExitedException\u001b[0m: process 6 terminated with exit code 17"
          ]
        }
      ]
    }
  ]
}